{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started This repository of notes is intended for quick reference, especially for beginners. This is not a textbook. Focus of each note is to provide minimal, dense, clutterfree info for the reader to get started. To contribute, submit a pull request. Here's a step by step guide: Step 1: Setting up First you need to have a clean linux operating system setup. You can install Ubuntu 20.04 on VirtualBox or VMWare workstation if you have a laptop with 4-8GB RAM. Install Visual Studio Code or any other code editor you prefer. Install Git on your laptop. Sign up for an account on Github. Then setup SSH access. Step 2: Working locally Fork the original repository to your own account. Clone your forked repository to your laptop. Install python-virtualenv & setup a virtualenv with name 'venv' in the code folder. Activate the virtualenv and install the pip dependencies . Start the local development server with: mkdocs serve Step 3: Submit a pull request Open the folder in the editor. Understand how material for mkdocs works. Make edits in the docs folder to edit the contents. Navigate to your local deployment to test the changes. Keep making changes till you are satisfied. Then commit the changes . Push to your github account with git push . Navigate to the forked repository in your github account using your browser and submit a pull request to the original repository. Raise a github issue on the original repository , if you face any issues.","title":"Getting started"},{"location":"#getting-started","text":"This repository of notes is intended for quick reference, especially for beginners. This is not a textbook. Focus of each note is to provide minimal, dense, clutterfree info for the reader to get started. To contribute, submit a pull request. Here's a step by step guide:","title":"Getting Started"},{"location":"#step-1-setting-up","text":"First you need to have a clean linux operating system setup. You can install Ubuntu 20.04 on VirtualBox or VMWare workstation if you have a laptop with 4-8GB RAM. Install Visual Studio Code or any other code editor you prefer. Install Git on your laptop. Sign up for an account on Github. Then setup SSH access.","title":"Step 1: Setting up"},{"location":"#step-2-working-locally","text":"Fork the original repository to your own account. Clone your forked repository to your laptop. Install python-virtualenv & setup a virtualenv with name 'venv' in the code folder. Activate the virtualenv and install the pip dependencies . Start the local development server with: mkdocs serve","title":"Step 2: Working locally"},{"location":"#step-3-submit-a-pull-request","text":"Open the folder in the editor. Understand how material for mkdocs works. Make edits in the docs folder to edit the contents. Navigate to your local deployment to test the changes. Keep making changes till you are satisfied. Then commit the changes . Push to your github account with git push . Navigate to the forked repository in your github account using your browser and submit a pull request to the original repository. Raise a github issue on the original repository , if you face any issues.","title":"Step 3: Submit a pull request"},{"location":"guidelines/","text":"Guidelines Composing a note Don't try to give a detailed description of a concept or a technology. Just one easy to understand line should be sufficient. Instead, focus on the detailed steps to get started. For example: how to install the package or what command to run or what file to look for etc. Don't explain anything in too much detail. Keep the info concise. Each point must not be more than 3-4 sentences. Add links to external sources for elaborate reading. But summarize the external source in the note. Link to reliable sources as much as possible like other documentation sites, stackoverflow etc. Avoid linking to random articles. Those articles may be taken down or the link may be changed or the content may be changed, which leads to broken / misleading information. In such cases, try to avoid the link by summarizing the necessary info directly in the note. Link to other notes as much as possible. As this repository is built for reference, each note may be read independently. Having a quick link to another note that clarifies a concept or gives background is very helpful. Formatting a note Separate points at the first level using paragraphs. Separate points at the second level using bullets. Use H2 or ## for side headings. Keep side headings short. Avoid long descriptive sentences as the side headings will be used to create the Table of Contents on the right of the page. Avoid \":\" at the end of headings. Use \":\" for bullet point descriptions. Use numbered bullets for steps or other ordered lists. Use '-' for unordered lists. No space before a comma. Space always after a comma. All bullets should start with a capital letter. Basics ;-)","title":"Guidelines"},{"location":"guidelines/#guidelines","text":"","title":"Guidelines"},{"location":"guidelines/#composing-a-note","text":"Don't try to give a detailed description of a concept or a technology. Just one easy to understand line should be sufficient. Instead, focus on the detailed steps to get started. For example: how to install the package or what command to run or what file to look for etc. Don't explain anything in too much detail. Keep the info concise. Each point must not be more than 3-4 sentences. Add links to external sources for elaborate reading. But summarize the external source in the note. Link to reliable sources as much as possible like other documentation sites, stackoverflow etc. Avoid linking to random articles. Those articles may be taken down or the link may be changed or the content may be changed, which leads to broken / misleading information. In such cases, try to avoid the link by summarizing the necessary info directly in the note. Link to other notes as much as possible. As this repository is built for reference, each note may be read independently. Having a quick link to another note that clarifies a concept or gives background is very helpful.","title":"Composing a note"},{"location":"guidelines/#formatting-a-note","text":"Separate points at the first level using paragraphs. Separate points at the second level using bullets. Use H2 or ## for side headings. Keep side headings short. Avoid long descriptive sentences as the side headings will be used to create the Table of Contents on the right of the page. Avoid \":\" at the end of headings. Use \":\" for bullet point descriptions. Use numbered bullets for steps or other ordered lists. Use '-' for unordered lists. No space before a comma. Space always after a comma. All bullets should start with a capital letter. Basics ;-)","title":"Formatting a note"},{"location":"stuck/","text":"Getting Stuck Stuckness shouldn't be avoided. It's the psychic predecessor of all real understanding. An egoless acceptance of stuckness is a key to an understanding of all Quality, in mechanical work as in other endeavors. It's this understanding of Quality as revealed by stuckness which so often makes self-taught mechanics so superior to institute-trained men who have learned how to handle everything except a new situation. -- Zen and the art of motorcycle maintenance (Robert M. Pirsig) This is where the real fun begins. Google Always, the first thing to do when stuck is ask google. Google for: error messages, keywords, describing the problem. Google in 5-6 different ways. Don't expect exact solutions. Look for posts that explain what is going on, so that you can understand enough to solve your issue. Stackoverflow results are most useful. Documentation sites next. Github issue discussions third. Finally personal blogs. Experiment As you keep learning more from googling, try experimenting on your own as well. Add print messages generously. This is the direct and most useful method. If you're on the browser, open developer tools and setup breakpoints. Try breaking your change into smaller changes and go step by step to see which step is the issue. Keep commits as small as possible. Even single lines. Test before every commit. So, that when you have an error, you have a smaller surface area for the bug to hide. When you are not sure what is happening under the hood, experiment to understand even if there is no bug. That understanding will pay disproportionately in the long run. Experimenting is the most fun way of learning things. Get out of the box Switch off for the day and start afresh the next day. Try to explain your problem to a friend. Just going over the problem again help see something you might have missed or come up with new ideas. Write a note on what got you stuck and the approaches you tried. Ask for help As a last effort, if you can't find any info through google and your own experiments haven't been insightful, you can ask for help. But, 99.99% of the time, this shouldn't be needed.","title":"Getting stuck"},{"location":"stuck/#getting-stuck","text":"Stuckness shouldn't be avoided. It's the psychic predecessor of all real understanding. An egoless acceptance of stuckness is a key to an understanding of all Quality, in mechanical work as in other endeavors. It's this understanding of Quality as revealed by stuckness which so often makes self-taught mechanics so superior to institute-trained men who have learned how to handle everything except a new situation. -- Zen and the art of motorcycle maintenance (Robert M. Pirsig) This is where the real fun begins.","title":"Getting Stuck"},{"location":"stuck/#google","text":"Always, the first thing to do when stuck is ask google. Google for: error messages, keywords, describing the problem. Google in 5-6 different ways. Don't expect exact solutions. Look for posts that explain what is going on, so that you can understand enough to solve your issue. Stackoverflow results are most useful. Documentation sites next. Github issue discussions third. Finally personal blogs.","title":"Google"},{"location":"stuck/#experiment","text":"As you keep learning more from googling, try experimenting on your own as well. Add print messages generously. This is the direct and most useful method. If you're on the browser, open developer tools and setup breakpoints. Try breaking your change into smaller changes and go step by step to see which step is the issue. Keep commits as small as possible. Even single lines. Test before every commit. So, that when you have an error, you have a smaller surface area for the bug to hide. When you are not sure what is happening under the hood, experiment to understand even if there is no bug. That understanding will pay disproportionately in the long run. Experimenting is the most fun way of learning things.","title":"Experiment"},{"location":"stuck/#get-out-of-the-box","text":"Switch off for the day and start afresh the next day. Try to explain your problem to a friend. Just going over the problem again help see something you might have missed or come up with new ideas. Write a note on what got you stuck and the approaches you tried.","title":"Get out of the box"},{"location":"stuck/#ask-for-help","text":"As a last effort, if you can't find any info through google and your own experiments haven't been insightful, you can ask for help. But, 99.99% of the time, this shouldn't be needed.","title":"Ask for help"},{"location":"Networking/overview/","text":"Networking Network Layers The network layer (Layer 3) of OSI controls the exchange of data packets, as these cannot be directly routed to the receiver and therefore have to be provided with routing nodes. The data packets are then transferred from node to node until they reach their target. To implement this, the network layer identifies the individual network nodes, sets up and clears connection channels, and takes care of routing and data flow control. When sending the packets, addresses are evaluated, and the data is routed through the network from node to node. There is usually no processing of the data in the layers above the L3 in the nodes. Based on the addresses, the routing and the construction of routing tables are done. Ports Ports are numerical identifiers (ranging from 0 to 65535) used in networking to distinguish different services or applications on the same server. If your web server is behind a router or firewall, you might need to set up port forwarding to direct incoming traffic on specific ports (like 80 or 443) to your server. Use tools like netstat or ss to check which ports are currently in use on your server and manage them accordingly. Ports use a protocol to direct traffic from them. Ports can be used with either the Transmission Control Protocol (TCP) or the User Datagram Protocol (UDP) . Network Address Translation (NAT) : Often works with port forwarding to enable multiple devices on a local network to share a single public IP address while directing incoming requests to the correct internal device. IP Addresses Each host in the network can be identified by the so-called Media Access Control address (MAC). This would allow data exchange within this one network. If the remote host is located in another network, knowledge of the MAC address is not enough to establish a connection. Addressing on the Internet is done via the IPv4 and/or IPv6 address, which is made up of the network address and the host address . The IP network blocks were divided into classes A - E. Subnetting A further separation of these classes into small networks is done with the help of subnetting . This separation is done using the netmasks, which is as long as an IPv4 address. As with classes, it describes which bit positions within the IP address act as network part or host part. Network and Gateway Addresses The two additional IPs added in the IPs column are reserved for the so-called network address and the broadcast address. Another important role plays the default gateway, which is the name for the IPv4 address of the router that couples networks and systems with different protocols and manages addresses and transmission methods. It is common for the default gateway to be assigned the first or last assignable IPv4 address in a subnet. Broadcast Address The broadcast IP address's task is to connect all devices in a network with each other. Broadcast in a network is a message that is transmitted to all participants of a network and does not require any response. In this way, a host sends a data packet to all other participants of the network simultaneously and, in doing so, communicates its IP address, which the receivers can use to contact it. This is the last IPv4 address that is used for the broadcast. HTTP The diagram above presents the anatomy of an HTTP request at a very high level. The first time a user enters the URL (inlanefreight.com) into the browser, it sends a request to a DNS (Domain Name Resolution) server to resolve the domain and get its IP. The DNS server looks up the IP address for inlanefreight.com and returns it. All domain names need to be resolved this way, as a server can't communicate without an IP address. Our browsers usually first look up records in the local /etc/hosts file, and if the requested domain does not exist within it, then they would contact other DNS servers. A request is sent to port 80 first, which is the unencrypted HTTP protocol . The server detects this and redirects the client to secure HTTPS port 443 instead. This is done via the 301 Moved Permanently response code. HTTP Response codes Signal Description GET Requests a specific resource. Additional data can be passed to the server via query strings in the URL (e.g. ?param=value). POST Sends data to the server. It can handle multiple types of input, such as text, PDFs, and other forms of binary data. This data is appended in the request body present after the headers. The POST method is commonly used when sending information (e.g. forms/logins) or uploading data to a website, such as images or documents. HEAD Requests the headers that would be returned if a GET request was made to the server. It doesn't return the request body and is usually made to check the response length before downloading resources. PUT Creates new resources on the server. Allowing this method without proper controls can lead to uploading malicious resources. DELETE Deletes an existing resource on the webserver. If not properly secured, can lead to Denial of Service (DoS) by deleting critical files on the web server. OPTIONS Returns information about the server, such as the methods accepted by it. PATCH Applies partial modifications to the resource at the specified location.","title":"Overview"},{"location":"Networking/overview/#networking","text":"","title":"Networking"},{"location":"Networking/overview/#network-layers","text":"The network layer (Layer 3) of OSI controls the exchange of data packets, as these cannot be directly routed to the receiver and therefore have to be provided with routing nodes. The data packets are then transferred from node to node until they reach their target. To implement this, the network layer identifies the individual network nodes, sets up and clears connection channels, and takes care of routing and data flow control. When sending the packets, addresses are evaluated, and the data is routed through the network from node to node. There is usually no processing of the data in the layers above the L3 in the nodes. Based on the addresses, the routing and the construction of routing tables are done.","title":"Network Layers"},{"location":"Networking/overview/#ports","text":"Ports are numerical identifiers (ranging from 0 to 65535) used in networking to distinguish different services or applications on the same server. If your web server is behind a router or firewall, you might need to set up port forwarding to direct incoming traffic on specific ports (like 80 or 443) to your server. Use tools like netstat or ss to check which ports are currently in use on your server and manage them accordingly. Ports use a protocol to direct traffic from them. Ports can be used with either the Transmission Control Protocol (TCP) or the User Datagram Protocol (UDP) . Network Address Translation (NAT) : Often works with port forwarding to enable multiple devices on a local network to share a single public IP address while directing incoming requests to the correct internal device.","title":"Ports"},{"location":"Networking/overview/#ip-addresses","text":"Each host in the network can be identified by the so-called Media Access Control address (MAC). This would allow data exchange within this one network. If the remote host is located in another network, knowledge of the MAC address is not enough to establish a connection. Addressing on the Internet is done via the IPv4 and/or IPv6 address, which is made up of the network address and the host address . The IP network blocks were divided into classes A - E.","title":"IP Addresses"},{"location":"Networking/overview/#subnetting","text":"A further separation of these classes into small networks is done with the help of subnetting . This separation is done using the netmasks, which is as long as an IPv4 address. As with classes, it describes which bit positions within the IP address act as network part or host part.","title":"Subnetting"},{"location":"Networking/overview/#network-and-gateway-addresses","text":"The two additional IPs added in the IPs column are reserved for the so-called network address and the broadcast address. Another important role plays the default gateway, which is the name for the IPv4 address of the router that couples networks and systems with different protocols and manages addresses and transmission methods. It is common for the default gateway to be assigned the first or last assignable IPv4 address in a subnet.","title":"Network and Gateway Addresses"},{"location":"Networking/overview/#broadcast-address","text":"The broadcast IP address's task is to connect all devices in a network with each other. Broadcast in a network is a message that is transmitted to all participants of a network and does not require any response. In this way, a host sends a data packet to all other participants of the network simultaneously and, in doing so, communicates its IP address, which the receivers can use to contact it. This is the last IPv4 address that is used for the broadcast.","title":"Broadcast Address"},{"location":"Networking/overview/#http","text":"The diagram above presents the anatomy of an HTTP request at a very high level. The first time a user enters the URL (inlanefreight.com) into the browser, it sends a request to a DNS (Domain Name Resolution) server to resolve the domain and get its IP. The DNS server looks up the IP address for inlanefreight.com and returns it. All domain names need to be resolved this way, as a server can't communicate without an IP address. Our browsers usually first look up records in the local /etc/hosts file, and if the requested domain does not exist within it, then they would contact other DNS servers. A request is sent to port 80 first, which is the unencrypted HTTP protocol . The server detects this and redirects the client to secure HTTPS port 443 instead. This is done via the 301 Moved Permanently response code.","title":"HTTP"},{"location":"Networking/overview/#http-response-codes","text":"Signal Description GET Requests a specific resource. Additional data can be passed to the server via query strings in the URL (e.g. ?param=value). POST Sends data to the server. It can handle multiple types of input, such as text, PDFs, and other forms of binary data. This data is appended in the request body present after the headers. The POST method is commonly used when sending information (e.g. forms/logins) or uploading data to a website, such as images or documents. HEAD Requests the headers that would be returned if a GET request was made to the server. It doesn't return the request body and is usually made to check the response length before downloading resources. PUT Creates new resources on the server. Allowing this method without proper controls can lead to uploading malicious resources. DELETE Deletes an existing resource on the webserver. If not properly secured, can lead to Denial of Service (DoS) by deleting critical files on the web server. OPTIONS Returns information about the server, such as the methods accepted by it. PATCH Applies partial modifications to the resource at the specified location.","title":"HTTP Response codes"},{"location":"backend/","text":"Backend: Overview API Understanding REST APIs Setting up Flask API Authentication Working with JSON data Database Understanding RDBMS Setting up an SQLite database SQLite vs MySQL Database backup Basics of SQL Working with DB: SQLAlchemy Flask SQL Alchemy Working with Models Migrating a DB: flask-migrate Architecture Monolith vs Microservice Orchestration vs Choreography Resource CRUD","title":"Overview"},{"location":"backend/#backend-overview","text":"","title":"Backend: Overview"},{"location":"backend/#api","text":"Understanding REST APIs Setting up Flask API Authentication Working with JSON data","title":"API"},{"location":"backend/#database","text":"Understanding RDBMS Setting up an SQLite database SQLite vs MySQL Database backup Basics of SQL Working with DB: SQLAlchemy Flask SQL Alchemy Working with Models Migrating a DB: flask-migrate","title":"Database"},{"location":"backend/#architecture","text":"Monolith vs Microservice Orchestration vs Choreography Resource CRUD","title":"Architecture"},{"location":"backend/flask/","text":"Flask","title":"Flask"},{"location":"backend/flask/#flask","text":"","title":"Flask"},{"location":"basics/","text":"Basics: Overview Each point in this page needs to be converted to a separate note. Please contribute with a pull request. Operating System Installing a functional UNIX/Linux operating system. Getting an overview of the filesystem Understanding file permissions, users & PATH Installing software packages that you need Shell Commands ls, cp, mv, rm, mkdir, rmdir, chmod, cat, export . .. | > >> & grep, find, sh, ps, top, less, tail, alias ssh, scp Git Creating account on github with SSH access Git Workflow: Using version control git basics: clone, pull, status, diff, add, commit, reset, push Working with branches: checkout, rebase, merge Docs mkdocs: The documentation system Markdown: The syntax used by mkdocs for creating content Python Virtualenv: Clean separate environment for python projects pip: Most popular python package manager","title":"Overview"},{"location":"basics/#basics-overview","text":"Each point in this page needs to be converted to a separate note. Please contribute with a pull request.","title":"Basics: Overview"},{"location":"basics/#operating-system","text":"Installing a functional UNIX/Linux operating system. Getting an overview of the filesystem Understanding file permissions, users & PATH Installing software packages that you need","title":"Operating System"},{"location":"basics/#shell-commands","text":"ls, cp, mv, rm, mkdir, rmdir, chmod, cat, export . .. | > >> & grep, find, sh, ps, top, less, tail, alias ssh, scp","title":"Shell Commands"},{"location":"basics/#git","text":"Creating account on github with SSH access Git Workflow: Using version control git basics: clone, pull, status, diff, add, commit, reset, push Working with branches: checkout, rebase, merge","title":"Git"},{"location":"basics/#docs","text":"mkdocs: The documentation system Markdown: The syntax used by mkdocs for creating content","title":"Docs"},{"location":"basics/#python","text":"Virtualenv: Clean separate environment for python projects pip: Most popular python package manager","title":"Python"},{"location":"basics/markdown/","text":"Markdown Markdown is a human readable plain text format for creating HTML content. A markdown file with a .md extension can be converted to a .html file which can be rendered / viewed in a web browser. A lot of online web tools like forums, blogs, static site generators and other websites like GitHub have support for Markdown. A block of markdown can be converted to a block of HTML. For example: # Title ### Sub title gets converted to: < h1 > Title </ h1 > < h3 > Sub title </ h3 > mkdocs mkdocs the documentation generator used for this project also uses Markdown for its content. So, we can write notes in markdown and mkdocs will convert that note into structured HTML pages that can be read on a browser. Cheat sheet A frequently used list of markdown elements Element Markdown Syntax Heading # H1 ## H2 ### H3 Bold __bold text__ **bold text** Italic _italicized text_ *italicized text* Blockquote > blockquote Ordered List 1. First item 2. Second item 3. Third item Unordered List - First item - Second item - Third item Code 'code' Horizontal Rule --- Link [Google](https://www.google.com) Table | Syntax | Description | | ------ | ----------- | | Header | Title | | Paragraph | Text | Task List - [x] Write code - [ ] Update the website - [ ] Contact the manager References Basic Writing and formatting syntax Advanced formatting Writing on GitHub","title":"Markdown"},{"location":"basics/markdown/#markdown","text":"Markdown is a human readable plain text format for creating HTML content. A markdown file with a .md extension can be converted to a .html file which can be rendered / viewed in a web browser. A lot of online web tools like forums, blogs, static site generators and other websites like GitHub have support for Markdown. A block of markdown can be converted to a block of HTML. For example: # Title ### Sub title gets converted to: < h1 > Title </ h1 > < h3 > Sub title </ h3 > mkdocs mkdocs the documentation generator used for this project also uses Markdown for its content. So, we can write notes in markdown and mkdocs will convert that note into structured HTML pages that can be read on a browser.","title":"Markdown"},{"location":"basics/markdown/#cheat-sheet","text":"A frequently used list of markdown elements Element Markdown Syntax Heading # H1 ## H2 ### H3 Bold __bold text__ **bold text** Italic _italicized text_ *italicized text* Blockquote > blockquote Ordered List 1. First item 2. Second item 3. Third item Unordered List - First item - Second item - Third item Code 'code' Horizontal Rule --- Link [Google](https://www.google.com) Table | Syntax | Description | | ------ | ----------- | | Header | Title | | Paragraph | Text | Task List - [x] Write code - [ ] Update the website - [ ] Contact the manager","title":"Cheat sheet"},{"location":"basics/markdown/#references","text":"Basic Writing and formatting syntax Advanced formatting Writing on GitHub","title":"References"},{"location":"basics/mkdocs/","text":"MkDocs Mkdocs is a static site generator that is mainly used for documentation. mkdocs \"builds\" (or generates) HTML documentation from content files written in Markdown . It comes with a built in development server to preview the documentation locally and a feature to deploy a live version to Github Pages . It can be configured using a single YAML file mkdocs.yml present in the root directory. Requirements Python (as MkDocs is a Python package) To check if python is installed on your system run python --version in the command line. If the output is something like Python 3.10.6 then you have python installed, skip to pip. If the output is something like Command 'python' not found,... then run sudo apt-get install python3 and follow the commands to get python installed in your system. pip (most common python package manager) To check if you already have pip installed run pip --version in the command line. If the output is something like pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10) then you have pip installed, skip to MkDocs installation. If the output is something like Command 'pip' not found,... then first run sudo apt update then run sudo apt install python3-pip , confirm for the pip installation. TODO: Installing python & pip should be moved into a separate note as it may be referred by multiple sources. DRY. TODO: Add instructions to setup a virtualenv to maintain a clean environment Installation To install mkdocs run pip install mkdocs , to confirm the installation run mkdocs --version . Then you require to install material (theme of mkdocs used in the project) using pip install mkdocs-material run mkdocs serve in the project directory to start a local development server. If you are able to see the project deployed then installation was successful. Workflow In the project folder there is a docs folder which contains all the markdown files used to build the documentation, by convention docs/index.md is the homepage. You can add content by editing the files or adding a new file. Below is step by step workflow for adding & publishing a new note: Add a note: First check under which directory in docs you need to add the file, you can do so by checking the note's place in the site tree, which is mentioned in the nav: in the mkdocs.yml file. Add and edit note_name.md in the proper directory and save it. Preview the note: run mkdocs serve it starts a local development server to see how the site will look published. after every edit just save your edits locally and the page auto-updates itself. Publish to GitHub Pages: once you are done making edits on the project you can publish the project on github pages, run mkdocs gh-deploy to do so. this will build the docs and commit them to gh-pages branch and push the gh-pages branch to GitHub and documentation should appear at <username>.github.io/<repository> . mkdocs serve doesn't generate the build folder. It directly serves from the markdown files. The build folder site can be generated by using mkdocs build , if necessary. mkdocs gh-deploy builds the documentation and adds all the build output files to a separate gh-pages branch and commits and pushes to GitHub triggering a publish. mkdocs.yml A quick reference for key settings: site_name is displayed on the top left of the documentation site. theme sets the theme name and theme specific configuration. Learn more. nav defines the structure of the sidebar in the \" - Title: ./file/path\" format.","title":"mkdocs"},{"location":"basics/mkdocs/#mkdocs","text":"Mkdocs is a static site generator that is mainly used for documentation. mkdocs \"builds\" (or generates) HTML documentation from content files written in Markdown . It comes with a built in development server to preview the documentation locally and a feature to deploy a live version to Github Pages . It can be configured using a single YAML file mkdocs.yml present in the root directory.","title":"MkDocs"},{"location":"basics/mkdocs/#requirements","text":"Python (as MkDocs is a Python package) To check if python is installed on your system run python --version in the command line. If the output is something like Python 3.10.6 then you have python installed, skip to pip. If the output is something like Command 'python' not found,... then run sudo apt-get install python3 and follow the commands to get python installed in your system. pip (most common python package manager) To check if you already have pip installed run pip --version in the command line. If the output is something like pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10) then you have pip installed, skip to MkDocs installation. If the output is something like Command 'pip' not found,... then first run sudo apt update then run sudo apt install python3-pip , confirm for the pip installation. TODO: Installing python & pip should be moved into a separate note as it may be referred by multiple sources. DRY. TODO: Add instructions to setup a virtualenv to maintain a clean environment","title":"Requirements"},{"location":"basics/mkdocs/#installation","text":"To install mkdocs run pip install mkdocs , to confirm the installation run mkdocs --version . Then you require to install material (theme of mkdocs used in the project) using pip install mkdocs-material run mkdocs serve in the project directory to start a local development server. If you are able to see the project deployed then installation was successful.","title":"Installation"},{"location":"basics/mkdocs/#workflow","text":"In the project folder there is a docs folder which contains all the markdown files used to build the documentation, by convention docs/index.md is the homepage. You can add content by editing the files or adding a new file. Below is step by step workflow for adding & publishing a new note: Add a note: First check under which directory in docs you need to add the file, you can do so by checking the note's place in the site tree, which is mentioned in the nav: in the mkdocs.yml file. Add and edit note_name.md in the proper directory and save it. Preview the note: run mkdocs serve it starts a local development server to see how the site will look published. after every edit just save your edits locally and the page auto-updates itself. Publish to GitHub Pages: once you are done making edits on the project you can publish the project on github pages, run mkdocs gh-deploy to do so. this will build the docs and commit them to gh-pages branch and push the gh-pages branch to GitHub and documentation should appear at <username>.github.io/<repository> . mkdocs serve doesn't generate the build folder. It directly serves from the markdown files. The build folder site can be generated by using mkdocs build , if necessary. mkdocs gh-deploy builds the documentation and adds all the build output files to a separate gh-pages branch and commits and pushes to GitHub triggering a publish.","title":"Workflow"},{"location":"basics/mkdocs/#mkdocsyml","text":"A quick reference for key settings: site_name is displayed on the top left of the documentation site. theme sets the theme name and theme specific configuration. Learn more. nav defines the structure of the sidebar in the \" - Title: ./file/path\" format.","title":"mkdocs.yml"},{"location":"basics/python-virtualenv/","text":"Python Virtual Environments Virtualenv is a tool which is used to create and manage Virtual Environments within python. A Virtual Environment is like a new isolated installation of python. Any package that you install will be downloaded and available only within the environment. The environment can easily be activated and deactivated on demand. Virtual Environments solves two major issues Version conflicts amongst the existing packages and the new packages Tracking only the packages which are used by the project Without virtual environments all the packages for every project that you make will be installed on your base python installation and clusters it, this makes it really hard to find the specific packages used in a project and sometimes different projects may require different versions of the same package which arises version conflict issues. Installation To check if virtualenv is installed on your system run virtualenv --version in the command line. If the output is something like virtualenv 20.16.5 from ... then you have virtualenv installed, skip to creation. If the output is something like Command 'virtualenv' not found ... you can install it via your package manager sudo apt-get install python3-virtualenv or via pip pip install virtualenv . venv vs virtualenv venv is a minimal working version of virtualenv which has been incorporated to the python standard library. If you do not wish to install virtualenv, you can use python -m venv instead of virtualenv . ex - python -m venv venv to create a virtual environment named venv. Creation After we have installed virtualenv, now we should create the virtual environment using the command virtualenv <NAME_OF_YOUR_ENVIRONMENT> . The name you specify is going to be the name of the environment. After running the command, a folder with that name will be created which contains all the files and packages installed in that virtual environment. After creating the virtual environment you should add it to the .gitignore file so that it does not get uploaded. For simplicity venv shall be used as the name for the environment as it is the usual name given for a virtual environment by convention and is pre-added to the .gitignore file. So the command would be virtualenv venv Activation After creating the environment, it must be activated in order to use the isolated environment. Due to the way POSIX derived shells work in Linux, it should be activated by sourcing the created setup file located at <NAME_OF_YOUR_ENVIRONMENT>/bin/activate , which in our case can be done by running the command source venv/bin/activate . If you notice the commandline starts with (venv) that indicates, the virtual environment is successfully activated. This can be verified by running which python the output should be the location of the new python withing the venv. If it does not work, try rechecking your steps and see which steps you missed. If you are getting any error message refer to Getting Stuck .","title":"Virtualenv"},{"location":"basics/python-virtualenv/#python-virtual-environments","text":"Virtualenv is a tool which is used to create and manage Virtual Environments within python. A Virtual Environment is like a new isolated installation of python. Any package that you install will be downloaded and available only within the environment. The environment can easily be activated and deactivated on demand. Virtual Environments solves two major issues Version conflicts amongst the existing packages and the new packages Tracking only the packages which are used by the project Without virtual environments all the packages for every project that you make will be installed on your base python installation and clusters it, this makes it really hard to find the specific packages used in a project and sometimes different projects may require different versions of the same package which arises version conflict issues.","title":"Python Virtual Environments"},{"location":"basics/python-virtualenv/#installation","text":"To check if virtualenv is installed on your system run virtualenv --version in the command line. If the output is something like virtualenv 20.16.5 from ... then you have virtualenv installed, skip to creation. If the output is something like Command 'virtualenv' not found ... you can install it via your package manager sudo apt-get install python3-virtualenv or via pip pip install virtualenv . venv vs virtualenv venv is a minimal working version of virtualenv which has been incorporated to the python standard library. If you do not wish to install virtualenv, you can use python -m venv instead of virtualenv . ex - python -m venv venv to create a virtual environment named venv.","title":"Installation"},{"location":"basics/python-virtualenv/#creation","text":"After we have installed virtualenv, now we should create the virtual environment using the command virtualenv <NAME_OF_YOUR_ENVIRONMENT> . The name you specify is going to be the name of the environment. After running the command, a folder with that name will be created which contains all the files and packages installed in that virtual environment. After creating the virtual environment you should add it to the .gitignore file so that it does not get uploaded. For simplicity venv shall be used as the name for the environment as it is the usual name given for a virtual environment by convention and is pre-added to the .gitignore file. So the command would be virtualenv venv","title":"Creation"},{"location":"basics/python-virtualenv/#activation","text":"After creating the environment, it must be activated in order to use the isolated environment. Due to the way POSIX derived shells work in Linux, it should be activated by sourcing the created setup file located at <NAME_OF_YOUR_ENVIRONMENT>/bin/activate , which in our case can be done by running the command source venv/bin/activate . If you notice the commandline starts with (venv) that indicates, the virtual environment is successfully activated. This can be verified by running which python the output should be the location of the new python withing the venv. If it does not work, try rechecking your steps and see which steps you missed. If you are getting any error message refer to Getting Stuck .","title":"Activation"},{"location":"code/","text":"1. Use a clear and descriptive title for the PR. Avoid vague titles like Updated \u2014 they don\u2019t communicate what changed. A good PR title summarizes the intent of the changes. \u2705 Good examples: - Fix: Remove __pycache__ and .db files - Refactor: Rename component files for consistency - Chore: Clean up console.log statements 2. Don\u2019t commit __pycache__ or .pyc files. These are automatically generated by Python and should not be versioned. They clutter the repo and vary across environments. Add these to .gitignore : __pycache__/ *.pyc Set this in your environment to prevent local writes: export PYTHONDONTWRITEBYTECODE=1 3. Don\u2019t commit venv/ or .venv/ folders. Virtual environments are local to your machine. They contain OS-specific binaries and shouldn't be checked into git. Add to .gitignore : venv/ .venv/ 4. Don\u2019t include .db files or other binary files. SQLite or binary files are not portable, cause merge conflicts, and should never be committed. \u2705 Instead: Use seed scripts or migrations Add to .gitignore : *.db 5. Don\u2019t commit machine-dependent or generated files. Avoid committing: - IDE configs (e.g. .vscode/ ) - Build outputs ( *.log , *.bin , compiled files) - .DS_Store , .log , etc. Keep your commits clean and portable across systems. 6. Remove console.log() statements before submitting. These should not be left in the production code unless explicitly needed for logging. Use a logging utility if required. 7. Fix spelling mistakes in filenames and identifiers. Example: \u274c VedioPlayer.svelte \u2705 VideoPlayer.svelte Spelling mistakes hurt searchability and reflect poorly on code professionalism. 8. Use consistent capitalization for component files. Follow PascalCase for Svelte components: \u2705 Button.svelte \u274c button.svelte Backend python files should be all smalls. For eg: renders.py Multi word file names should be avoided. If absolutely needed, _ should be used for space. For eg: api_routes.py This avoids confusion and bugs on case-sensitive filesystems (e.g., Linux) and keeps the file structure consistent. 9. Follow RESTful naming conventions for API routes. We already discussed this during your interview assignment. Avoid actions like /getProjectDetails . Instead: Action Method RESTful Route Get user info GET /users/:id Create project POST /projects Update project PUT /projects/:id Delete a resource DELETE /resources/:id 10. Avoid Adding Unnecessary Dependencies. \u2757 Always avoid introducing plugins, architectural patterns, or external libraries unless absolutely necessary. Why it matters: - Every dependency increases maintenance and onboarding complexity - It adds abstraction that may not be justified for smaller projects - Makes debugging and testing more complex \u2705 For eg: Stick with Flask\u2019s built-in @app.route decorators unless: - No need to use flask-blueprints plugin - The project is large enough to benefit from modular separation (e.g., auth , user , admin ) - There is a real and current need for abstraction Before adding a dependency, ask: - Can this be done simply with what's already available? - Will this make the project harder to maintain? - Are there multiple contributors familiar with this tool? \ud83e\udde0 Keeping things simple makes the project easier to maintain and scale.","title":"Code"},{"location":"frontend/","text":"Frontend: Overview","title":"Overview"},{"location":"frontend/#frontend-overview","text":"","title":"Frontend: Overview"},{"location":"git/pr/","text":"","title":"Pr"},{"location":"git/workflow/","text":"Workflow Workflow is just the recommended way of using the version control. Version Control is a system that records changes to a file or set of files over time so that you can recall specific versions later. There are a lot of types of version controlled system, to manage this project we use git, a Distributed Version Control System. Git is flexible in how a user manage changes, which leads to different types of workflows. This project uses what you call is a Forking Git Workflow . Forking a repo Fork the organization repo to your github account. A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project. To fork a repo, just go to the site of the repo you want to fork, and click the Fork button on the page. Select an owner for the forked repo Choose whether to copy only the default branch or all branches to the new fork. Click Create fork. Cloning your forked repo Now, you have a personal remote copy of the repo. You need to bring this copy to your local machine so that you will be able to make changes. We do this by cloning the repo. To clone a fork, go to GitHub.com, navigate to your fork. Above the list of files, click on Code and then choose SSH option, copy the SSH URL for the repo. Now, open terminal and cd to where you want the cloned repo. Type git clone , and then paste the URL you copied earlier. It will look something like git clone git@github.com:YOUR_USERNAME/YOUR_FORK.git Press Enter . Your local clone will be created. Run git remote -v . You will see current configured remote repository for your fork. $ git remote -v > origin git@github.com:YOUR_USERNAME/YOUR_FORK.git ( fetch ) > origin git@github.com:YOUR_USERNAME/YOUR_FORK.git ( push ) origin is auto-generated and points at your fork of the repo(parent repo). When you make new changes to the code base, origin is where you will push them. Keeping your fork updated Eventually, the original repo will have changes made to it, so you need a way to pull these updates. In order to do so, you need to add a remote that points to it. Traditionally, we call this upstream . On GitHub.com, navigate to the original repo. Above the list of files, click on Code and then choose SSH option, copy the SSH URL for the repo. Open your clone of the fork in Terminal. Type git remote add upstream and then paste the URL and press Enter . It will look like $ git remote add upstream git@github.com:ORIGINAL_OWNER/REPO_NAME.git To verify run git remote -v . $ git remote -v > origin git@github.com:YOUR_USERNAME/YOUR_FORK.git ( fetch ) > origin git@github.com:YOUR_USERNAME/YOUR_FORK.git ( push ) > upstream git@github.com:ORIGINAL_OWNER/REPO_NAME.git ( fetch ) > upstream git@github.com:ORIGINAL_OWNER/REPO_NAME.git ( push ) Making Changes Before you make any changes, you should make a branch. Remember to never commit to main . The command git status will tell you what branch you are on. It is important that you never commit to main because main will be the branch that you will pull upstream changes from. Update main - Before you make any changes, first checkout main. git checkout main and pull in the latest changes git pull This will make it so that your changes are against the very latest main, which will reduce the likelihood of merge conflicts due to your changes conflicting with changes made by someone else. Create a branch - You should make a branch name that is short, descriptive, and unique, as people will use this to reference your changes if they want to pull them down on their own computer to test them. To create a branch, run git checkout -b branch-name . This will create a new branch and check it out. You can verify this with git status . Make your changes and commit them - Once you have created your branch, make your changes and commit them. Remember to keep your commits atomic, that is, each commit should represent a single unit of change. Also, remember to write helpful commit messages, so that someone can understand what the commit does just from the reading the message without having to read the diff . For example, at the command line, this might look like git add filename [ filename ... ] git commit This will open an editor where you can write your commit message. Push up your changes - Push your changes to your fork. Do this, by running git push origin feature-branch Make a pull request - Once you see your branch appear in GitHub, be sure to compare your branch with the original repo's main branch. This is where you can check for any merge conflicts that need to be resolved. After resolving, if it all looks good, make your pull request of the branch you want. Give your pull request a proper title and description(optional). Pushing additional changes - Once you have created a pull request, it will likely be reviewed and some additional fixes will be necessary. Do not create a pull request . Rather, simply make more commits to your branch and push them. They will be added to the pull request automatically. Once the pull request has been reviewed successfully, someone with push access to the main repo will merge it in. At this point you are done. You can checkout main and pull as described in Step 1 and your changes should be there. Important points You only need to clone and fork once per repo. Use git status often to check what branch you are on and see if you have any uncommitted changes. Be descriptive in your branch names, commit messages, and pull request title and descriptions. It is good idea to make a comment on the pull request whenever you commit more changes so people get notified that it is ready to be reviewed again, as many people have notifications off for commits on pull requests. References More on Version Control Some Distributed Git Workflows Workflow recommended by GitHub Contributing to a project in Distributed Git","title":"Workflow"},{"location":"git/workflow/#workflow","text":"Workflow is just the recommended way of using the version control. Version Control is a system that records changes to a file or set of files over time so that you can recall specific versions later. There are a lot of types of version controlled system, to manage this project we use git, a Distributed Version Control System. Git is flexible in how a user manage changes, which leads to different types of workflows. This project uses what you call is a Forking Git Workflow .","title":"Workflow"},{"location":"git/workflow/#forking-a-repo","text":"Fork the organization repo to your github account. A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project. To fork a repo, just go to the site of the repo you want to fork, and click the Fork button on the page. Select an owner for the forked repo Choose whether to copy only the default branch or all branches to the new fork. Click Create fork.","title":"Forking a repo"},{"location":"git/workflow/#cloning-your-forked-repo","text":"Now, you have a personal remote copy of the repo. You need to bring this copy to your local machine so that you will be able to make changes. We do this by cloning the repo. To clone a fork, go to GitHub.com, navigate to your fork. Above the list of files, click on Code and then choose SSH option, copy the SSH URL for the repo. Now, open terminal and cd to where you want the cloned repo. Type git clone , and then paste the URL you copied earlier. It will look something like git clone git@github.com:YOUR_USERNAME/YOUR_FORK.git Press Enter . Your local clone will be created. Run git remote -v . You will see current configured remote repository for your fork. $ git remote -v > origin git@github.com:YOUR_USERNAME/YOUR_FORK.git ( fetch ) > origin git@github.com:YOUR_USERNAME/YOUR_FORK.git ( push ) origin is auto-generated and points at your fork of the repo(parent repo). When you make new changes to the code base, origin is where you will push them.","title":"Cloning your forked repo"},{"location":"git/workflow/#keeping-your-fork-updated","text":"Eventually, the original repo will have changes made to it, so you need a way to pull these updates. In order to do so, you need to add a remote that points to it. Traditionally, we call this upstream . On GitHub.com, navigate to the original repo. Above the list of files, click on Code and then choose SSH option, copy the SSH URL for the repo. Open your clone of the fork in Terminal. Type git remote add upstream and then paste the URL and press Enter . It will look like $ git remote add upstream git@github.com:ORIGINAL_OWNER/REPO_NAME.git To verify run git remote -v . $ git remote -v > origin git@github.com:YOUR_USERNAME/YOUR_FORK.git ( fetch ) > origin git@github.com:YOUR_USERNAME/YOUR_FORK.git ( push ) > upstream git@github.com:ORIGINAL_OWNER/REPO_NAME.git ( fetch ) > upstream git@github.com:ORIGINAL_OWNER/REPO_NAME.git ( push )","title":"Keeping your fork updated"},{"location":"git/workflow/#making-changes","text":"Before you make any changes, you should make a branch. Remember to never commit to main . The command git status will tell you what branch you are on. It is important that you never commit to main because main will be the branch that you will pull upstream changes from. Update main - Before you make any changes, first checkout main. git checkout main and pull in the latest changes git pull This will make it so that your changes are against the very latest main, which will reduce the likelihood of merge conflicts due to your changes conflicting with changes made by someone else. Create a branch - You should make a branch name that is short, descriptive, and unique, as people will use this to reference your changes if they want to pull them down on their own computer to test them. To create a branch, run git checkout -b branch-name . This will create a new branch and check it out. You can verify this with git status . Make your changes and commit them - Once you have created your branch, make your changes and commit them. Remember to keep your commits atomic, that is, each commit should represent a single unit of change. Also, remember to write helpful commit messages, so that someone can understand what the commit does just from the reading the message without having to read the diff . For example, at the command line, this might look like git add filename [ filename ... ] git commit This will open an editor where you can write your commit message. Push up your changes - Push your changes to your fork. Do this, by running git push origin feature-branch Make a pull request - Once you see your branch appear in GitHub, be sure to compare your branch with the original repo's main branch. This is where you can check for any merge conflicts that need to be resolved. After resolving, if it all looks good, make your pull request of the branch you want. Give your pull request a proper title and description(optional). Pushing additional changes - Once you have created a pull request, it will likely be reviewed and some additional fixes will be necessary. Do not create a pull request . Rather, simply make more commits to your branch and push them. They will be added to the pull request automatically. Once the pull request has been reviewed successfully, someone with push access to the main repo will merge it in. At this point you are done. You can checkout main and pull as described in Step 1 and your changes should be there. Important points You only need to clone and fork once per repo. Use git status often to check what branch you are on and see if you have any uncommitted changes. Be descriptive in your branch names, commit messages, and pull request title and descriptions. It is good idea to make a comment on the pull request whenever you commit more changes so people get notified that it is ready to be reviewed again, as many people have notifications off for commits on pull requests.","title":"Making Changes"},{"location":"git/workflow/#references","text":"More on Version Control Some Distributed Git Workflows Workflow recommended by GitHub Contributing to a project in Distributed Git","title":"References"},{"location":"os/linux/","text":"Linux Processes systemd : This daemon is an Init process started first and thus has the process ID (PID) 1 . This daemon monitors and takes care of the orderly starting and stopping of other services. All the background services can be listed using: systemctl list-units --type=service All running processes can be listed using ps -aux . To add any service to the SysV script(/lib/systemd/systemd-sysv-install) to tell the system to run this service after startup, we can link it with the following command: systemctl service_name start Automatically set a process to run in background by adding an & to the end of command. ping -c 10 www.google.com & We can use the jobs command to list all background processes. fg <ID> to bring a background process to foreground. Processes can be controlled using kill , pkill , pgrep , and killall . To interact with a process, we must send a signal to it. kill 9 <PID> The most commonly used are: Signal Description SIGHUP 1 This is sent to a process when the terminal that controls it is closed. SIGINT 2 Sent when a user presses [Ctrl] + C in the controlling terminal to interrupt a process. SIGQUIT 3 Sent when a user presses [Ctrl] + D to quit. SIGKILL 9 Immediately kill a process with no clean-up operations. SIGTERM 15 Program termination. SIGSTOP 19 Stop the program. It cannot be handled anymore. SIGTSTP 20 Sent when a user presses [Ctrl] + Z to request for a service to suspend. The user can handle it afterward. (Background a process) Threads A process can do more than one unit of work concurrently by creating one or more threads. These threads, being lightweight processes(LWP), can be spawned quickly. Therefore, a process can be single-threaded or multi-threaded. Use ps -eLf and check which process( PID ) is running how many threads( NLWP ) Any thread created within the process shares the same memory and resources of the process. In a single-threaded process, the process and thread are the same, as there\u2019s only one thing happening. Spawning a new thread within a process becomes cheap (in terms of the system resources) compared to starting a new process. Memory Paging Memory paging is a memory management scheme by which a computer stores and retrieves data from secondary storage for use in main memory. Memory Division into Pages and Frames: Pages : The process's virtual memory is divided into fixed-size blocks called pages. Frames : The physical memory (RAM) is divided into blocks of the same size called frames. Page Table: Each process has a page table that maps its virtual pages to the physical frames in RAM. The page table keeps track of where each page of the process's virtual memory is stored in physical memory. Address Translation: When a process accesses memory, the CPU translates the virtual address to a physical address using the page table. The virtual address consists of a page number and an offset within that page. The page number is used to find the corresponding frame number from the page table. Page Faults: If a process tries to access a page that is not currently in physical memory, a page fault occurs. The operating system then loads the required page from secondary storage (like a hard drive) into a free frame in physical memory. Benefits: Efficient Use of Memory : Paging allows for efficient use of physical memory by loading only the necessary pages into RAM, rather than the entire process. Isolation and Protection : It provides isolation between processes, as each process operates in its own virtual address space, which enhances security and stability. Process Forking The fork() system call is used for creating a new process in Unix/Linux. The child process created by the process that makes the fork() \u200b call. After child process creation, both parent and child process executes the next instruction. fork() \u200b\u200b will not take any arguments but return integer. Return Values: Return Value Description +ve The creation of child process is unsuccessful. 0 Returned to the newly created child process. -ve Returned to the parent or Caller. The value contains the process ID of the newly created child process. Race Condition A Race condition is a scenario that occurs in a multithreaded environment due to multiple threads sharing the same resource or executing the same piece of code. If not handled properly, this can lead to an undesirable situation, where the output state is dependent on the order of execution of the threads. In simple words, if two threads/processes try to execute the same operations at the same time, then these two threads are in a race , in which one thread has to lose. There are two types of race conditions in an OS: Read-modify-write : This kind of race condition happens when two processes read a value in a program and write back a new value. check-then-act : This race condition happens when two processes check a value on which they will take external action. Critical Section of a code: part that is executed by multiple threads. The ideal way to prevent a race-around condition is by controlling access to the critical section of your code. Race conditions can leave the system vulnerable to security attacks where attackers can tamper with the shared data. Mutual Exclusion (Mutex) A mutual exclusion ( mutex ) is a program object that prevents multiple threads from accessing the same shared resource simultaneously. A mutex ensures that the threads carry out their operations like the two threads, with each thread accessing the critical section of code one at a time. This way, they don't step over each other and return undesired results, but instead behave in a controlled and predictable manner. Locks A multithreaded program can specifically request a mutex for each shared resource from the underlying system. If a thread needs to access the resource, it must first verify whether the mutex for that resource is locked. If it is unlocked, the thread can execute the code's critical section. If it is locked, the system typically queues the thread until the mutex becomes unlocked. When this occurs, the thread can execute the critical section, while the mutex is again locked to prevent other threads from using the resource. Semaphores It is a process synchronization tool. A semaphore is an integer variable, shared among multiple processes. The main aim of using a semaphore is process synchronization and access control for a common resource in a concurrent environment. There are two types of semaphores: Binary semaphore Counting Semaphore A binary semaphore can have only two integer values: 0 or 1. It\u2019s simpler to implement and provides mutual exclusion. We can use a binary semaphore to solve the critical section problem. NOTE : A semaphore is a signaling mechanism where on the other hand, a mutex is a locking mechanism. A counting semaphore is again an integer value, which can range over an unrestricted domain. We can use it to resolve synchronization problems like resource allocation. Load Average Load average in Linux measures the usage of system resources. Understanding this metric helps sysadmins identify and troubleshoot performance issues. There are various tools to monitor the load average such as: uptime , top , w , cat /proc/loadavg , glances $ uptime 16:02:11 up 7:56, 3 users, load average: 1.06, 0.55, 0.37 The system is running from 7 hours and 56 mins since last boot. The load average for 3 users is: 1.06 for the past 1 minute. 0.55 for the past 5 minutes. 0.37 for the past 15 minutes. The results are calculated by dividing the number of running and waiting processes by the number of available CPU cores. Signals A signal is a kind of (usually software) interrupt, used to announce asynchronous events to a process. The name of a LINUX signal begins with \"SIG\". Although signals are numbered, we normally refer to them by their names. List all the signals that can be sent to any process. $ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX Kill a process with id = PID $ kill 9 <PID> Unix Domain Sockets Unix Domain Sockets (UDS) are a communication mechanism in Unix-based systems that enable data exchange between processes running on the same machine. They are used for inter-process communication (IPC) and offer several advantages over traditional network sockets when the communication does not need to cross the network boundary. Sockets provide a means of communication between processes, i.e. a way for them to exchange data. The way it usually works is that process_a has socket_x , process_b has socket_y , and the two sockets are connected. Each process can then use its socket to receive data from the other process and/or send data to the other process. Local Communication : Unlike network sockets, UDS only work within the same host. File System Integration : UDS are represented as files (for eg. /var/run/docker.sock ) in the file system. Performance : They are generally faster than TCP/IP sockets because they bypass the network stack and have lower overhead. Security : UDS benefit from Unix file system permissions, which can be used to restrict access.","title":"Linux"},{"location":"os/linux/#linux","text":"","title":"Linux"},{"location":"os/linux/#processes","text":"systemd : This daemon is an Init process started first and thus has the process ID (PID) 1 . This daemon monitors and takes care of the orderly starting and stopping of other services. All the background services can be listed using: systemctl list-units --type=service All running processes can be listed using ps -aux . To add any service to the SysV script(/lib/systemd/systemd-sysv-install) to tell the system to run this service after startup, we can link it with the following command: systemctl service_name start Automatically set a process to run in background by adding an & to the end of command. ping -c 10 www.google.com & We can use the jobs command to list all background processes. fg <ID> to bring a background process to foreground. Processes can be controlled using kill , pkill , pgrep , and killall . To interact with a process, we must send a signal to it. kill 9 <PID> The most commonly used are: Signal Description SIGHUP 1 This is sent to a process when the terminal that controls it is closed. SIGINT 2 Sent when a user presses [Ctrl] + C in the controlling terminal to interrupt a process. SIGQUIT 3 Sent when a user presses [Ctrl] + D to quit. SIGKILL 9 Immediately kill a process with no clean-up operations. SIGTERM 15 Program termination. SIGSTOP 19 Stop the program. It cannot be handled anymore. SIGTSTP 20 Sent when a user presses [Ctrl] + Z to request for a service to suspend. The user can handle it afterward. (Background a process)","title":"Processes"},{"location":"os/linux/#threads","text":"A process can do more than one unit of work concurrently by creating one or more threads. These threads, being lightweight processes(LWP), can be spawned quickly. Therefore, a process can be single-threaded or multi-threaded. Use ps -eLf and check which process( PID ) is running how many threads( NLWP ) Any thread created within the process shares the same memory and resources of the process. In a single-threaded process, the process and thread are the same, as there\u2019s only one thing happening. Spawning a new thread within a process becomes cheap (in terms of the system resources) compared to starting a new process.","title":"Threads"},{"location":"os/linux/#memory-paging","text":"Memory paging is a memory management scheme by which a computer stores and retrieves data from secondary storage for use in main memory. Memory Division into Pages and Frames: Pages : The process's virtual memory is divided into fixed-size blocks called pages. Frames : The physical memory (RAM) is divided into blocks of the same size called frames. Page Table: Each process has a page table that maps its virtual pages to the physical frames in RAM. The page table keeps track of where each page of the process's virtual memory is stored in physical memory. Address Translation: When a process accesses memory, the CPU translates the virtual address to a physical address using the page table. The virtual address consists of a page number and an offset within that page. The page number is used to find the corresponding frame number from the page table. Page Faults: If a process tries to access a page that is not currently in physical memory, a page fault occurs. The operating system then loads the required page from secondary storage (like a hard drive) into a free frame in physical memory. Benefits: Efficient Use of Memory : Paging allows for efficient use of physical memory by loading only the necessary pages into RAM, rather than the entire process. Isolation and Protection : It provides isolation between processes, as each process operates in its own virtual address space, which enhances security and stability.","title":"Memory Paging"},{"location":"os/linux/#process-forking","text":"The fork() system call is used for creating a new process in Unix/Linux. The child process created by the process that makes the fork() \u200b call. After child process creation, both parent and child process executes the next instruction. fork() \u200b\u200b will not take any arguments but return integer. Return Values: Return Value Description +ve The creation of child process is unsuccessful. 0 Returned to the newly created child process. -ve Returned to the parent or Caller. The value contains the process ID of the newly created child process.","title":"Process Forking"},{"location":"os/linux/#race-condition","text":"A Race condition is a scenario that occurs in a multithreaded environment due to multiple threads sharing the same resource or executing the same piece of code. If not handled properly, this can lead to an undesirable situation, where the output state is dependent on the order of execution of the threads. In simple words, if two threads/processes try to execute the same operations at the same time, then these two threads are in a race , in which one thread has to lose. There are two types of race conditions in an OS: Read-modify-write : This kind of race condition happens when two processes read a value in a program and write back a new value. check-then-act : This race condition happens when two processes check a value on which they will take external action. Critical Section of a code: part that is executed by multiple threads. The ideal way to prevent a race-around condition is by controlling access to the critical section of your code. Race conditions can leave the system vulnerable to security attacks where attackers can tamper with the shared data.","title":"Race Condition"},{"location":"os/linux/#mutual-exclusion-mutex","text":"A mutual exclusion ( mutex ) is a program object that prevents multiple threads from accessing the same shared resource simultaneously. A mutex ensures that the threads carry out their operations like the two threads, with each thread accessing the critical section of code one at a time. This way, they don't step over each other and return undesired results, but instead behave in a controlled and predictable manner.","title":"Mutual Exclusion (Mutex)"},{"location":"os/linux/#locks","text":"A multithreaded program can specifically request a mutex for each shared resource from the underlying system. If a thread needs to access the resource, it must first verify whether the mutex for that resource is locked. If it is unlocked, the thread can execute the code's critical section. If it is locked, the system typically queues the thread until the mutex becomes unlocked. When this occurs, the thread can execute the critical section, while the mutex is again locked to prevent other threads from using the resource.","title":"Locks"},{"location":"os/linux/#semaphores","text":"It is a process synchronization tool. A semaphore is an integer variable, shared among multiple processes. The main aim of using a semaphore is process synchronization and access control for a common resource in a concurrent environment. There are two types of semaphores: Binary semaphore Counting Semaphore A binary semaphore can have only two integer values: 0 or 1. It\u2019s simpler to implement and provides mutual exclusion. We can use a binary semaphore to solve the critical section problem. NOTE : A semaphore is a signaling mechanism where on the other hand, a mutex is a locking mechanism. A counting semaphore is again an integer value, which can range over an unrestricted domain. We can use it to resolve synchronization problems like resource allocation.","title":"Semaphores"},{"location":"os/linux/#load-average","text":"Load average in Linux measures the usage of system resources. Understanding this metric helps sysadmins identify and troubleshoot performance issues. There are various tools to monitor the load average such as: uptime , top , w , cat /proc/loadavg , glances $ uptime 16:02:11 up 7:56, 3 users, load average: 1.06, 0.55, 0.37 The system is running from 7 hours and 56 mins since last boot. The load average for 3 users is: 1.06 for the past 1 minute. 0.55 for the past 5 minutes. 0.37 for the past 15 minutes. The results are calculated by dividing the number of running and waiting processes by the number of available CPU cores.","title":"Load Average"},{"location":"os/linux/#signals","text":"A signal is a kind of (usually software) interrupt, used to announce asynchronous events to a process. The name of a LINUX signal begins with \"SIG\". Although signals are numbered, we normally refer to them by their names. List all the signals that can be sent to any process. $ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX Kill a process with id = PID $ kill 9 <PID>","title":"Signals"},{"location":"os/linux/#unix-domain-sockets","text":"Unix Domain Sockets (UDS) are a communication mechanism in Unix-based systems that enable data exchange between processes running on the same machine. They are used for inter-process communication (IPC) and offer several advantages over traditional network sockets when the communication does not need to cross the network boundary. Sockets provide a means of communication between processes, i.e. a way for them to exchange data. The way it usually works is that process_a has socket_x , process_b has socket_y , and the two sockets are connected. Each process can then use its socket to receive data from the other process and/or send data to the other process. Local Communication : Unlike network sockets, UDS only work within the same host. File System Integration : UDS are represented as files (for eg. /var/run/docker.sock ) in the file system. Performance : They are generally faster than TCP/IP sockets because they bypass the network stack and have lower overhead. Security : UDS benefit from Unix file system permissions, which can be used to restrict access.","title":"Unix Domain Sockets"}]}